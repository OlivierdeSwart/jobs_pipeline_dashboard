# ğŸ›  JOBS_PIPELINE_DASHBOARD

End-to-end modern data engineering pipeline to ingest job postings, transform with dbt, and visualize insights in Tableau. Deployable locally via Docker or in production on AWS using MWAA (Managed Workflows for Apache Airflow), S3, and Snowflake.

---

## ğŸ”§ Project Structure

JOBS_PIPELINE_DASHBOARD/
â”‚
â”œâ”€â”€ airflow/                    # Local Airflow project root
â”‚   â”œâ”€â”€ dags/                   # Local-only DAGs
â”‚   â”œâ”€â”€ config/                 # Configuration files
â”‚   â””â”€â”€ logs/                   # Airflow logs
â”‚
â”œâ”€â”€ mwaa-deploy/                # Production deployable setup for AWS MWAA
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ dbt/                # dbt project directory
â”‚   â”‚   â””â”€â”€ scripts/           # Python scripts used in the pipeline
â”‚   â”œâ”€â”€ requirements.txt        # MWAA dependencies
â”‚   â””â”€â”€ startup.sh              # Bootstrapping shell script
â”‚
â”œâ”€â”€ dbt_jobs/                   # dbt project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # Stage raw Snowflake tables
â”‚   â”‚   â”œâ”€â”€ dwh/                # Data warehouse layer
â”‚   â”‚   â””â”€â”€ dm/                 # Data marts
â”‚   â”œâ”€â”€ snapshots/              # Versioning for slowly changing dimensions
â”‚   â”œâ”€â”€ logs/                   # dbt logs
â”‚   â”œâ”€â”€ seeds/                  # Static CSV seed files
â”‚   â””â”€â”€ dbt_project.yml         # Project config
â”‚
â”œâ”€â”€ data_storage/               # Ingested NDJSON data from API
â”œâ”€â”€ data_analysis/              # Notebook-style EDA or analysis scripts
â”œâ”€â”€ scripts/                    # Finalized ingestion/transformation scripts
â”œâ”€â”€ scripts_old/                # Deprecated/archived versions
â”œâ”€â”€ logs/Screenshots/           # Pipeline and dashboard screenshots
â”‚
â”œâ”€â”€ .env                        # Environment variables
â”œâ”€â”€ docker-compose.yaml         # Local dev stack
â”œâ”€â”€ Dockerfile                  # Airflow image config
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ README.md                   # This file âœ¨
â””â”€â”€ 20250403_notes              # Project journal / scratchpad

---

## âš™ï¸ How It Works

### 1. Ingest Jobs from API â†’ S3
- `aws_1_api_to_storage.py`
- PythonOperator calls `run_ingest_all()` from `scripts/aws_1_api_to_storage.py`
- Stores raw NDJSON in S3 (or locally in dev mode)

### 2. Load S3 Data â†’ Snowflake
- `aws_2_s3_to_snowflake.py`
- PythonOperator calls `load_s3_to_snowflake()` with environment vars from `.env`
- Loads data into Snowflake raw tables

### 3. dbt: Transform in Snowflake
- `aws_3_jobs_sta_to_dwh.py`
- BashOperator calls dbt using secrets from AWS Secrets Manager
- Models include:
  - `jobs_base.sql`: Raw jobs cleaned
  - `jobs_current.sql`: Current open jobs
  - `fct_job_postings_lifecycle.sql`: New, updated, deleted logic

### 4. Combine in a Single DAG
- `aws_4_all_combined.py`
- Runs the full daily flow: API â†’ S3 â†’ Snowflake â†’ dbt

---

## ğŸš€ Deployment

### â–¶ï¸ Local (Docker Compose)
```bash
docker-compose up airflow-init
docker-compose up

	â€¢	Airflow UI: http://localhost:8080
	â€¢	Dags are mounted live from airflow/dags (dev) or mwaa-deploy/dags (prod)

â˜ï¸ AWS (MWAA)
	1.	Package up mwaa-deploy/ contents
	2.	Upload to S3 bucket configured for your MWAA environment
	3.	Ensure:
	â€¢	.env and secrets are in AWS Secrets Manager
	â€¢	Your Airflow environment has access to S3, Snowflake, and Secrets Manager
	4.	Use startup.sh for bootstrapping

â¸»

ğŸ“Š Dashboard

Built in Tableau using Snowflake as a live source.

Key dashboards:
	â€¢	Jobs Lifecycle: New, updated, archived jobs per day
	â€¢	Jobs KPIs: Avg open days, new jobs per day
	â€¢	Behavioral cohort potential via job updates

â¸»

ğŸ§  Notebooks, Notes, and Screenshots

Located in:
	â€¢	data_analysis/ â€“ EDA + Tableau prep
	â€¢	20250403_notes â€“ scratchpad for pipeline logic
	â€¢	logs/Screenshots/ â€“ proof of success + dashboard output

â¸»

ğŸªª Auth & Secrets

Secrets pulled securely from AWS Secrets Manager:
	â€¢	one-secret-to-rule-them-all â€“ contains Snowflake + dbt creds

â¸»

ğŸ“¦ Future Improvements
	â€¢	Add CI/CD for dbt with GitHub Actions
	â€¢	Incremental model logic
	â€¢	Snapshot coverage for full lifecycle tracking
	â€¢	Behavioral cohorts on job updates
	â€¢	Webhook/streaming ingestion

â¸»

ğŸ§  Author

Made by Olivier de Swart
olivierdeswart.com

â¸»

Screenshot
Description
05_airflow_dag_success.png
Successful local DAG execution
06_snowflake_first_ingested_data.png
First records in raw Snowflake table
07_dbt_models_done.png
dbt models compiled & run
10_tableau_viz.png
Final dashboard
13_dashboard_lifecycle.png
Lifecycle viz showing job states
