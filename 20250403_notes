20250403_notes

Blog concept: how to create a developer portfolio project

Think which skill(s) you want to showcase (DE)
Think of general idea (dashboard with job trends)
Think of tech stack, bonus points for review with AI (Airflow with Python, Snowflake DB, DBT, Tableau for dashboard)
	Also helpful to discuss repo/folder setup
Other things you want to showcase (star schema data modeling, STA/DWH/DM)
Take note of the questions you have along the way. Or you think other people might have
It's great if you can explain the "why" of what you're doing. (for me: performance and storage will be decreased. that's large amounts of money saved for data intensive companies. that's why data engineers have the right to exist in essence)

Start working. For me:
Find API with data: Remotive

https://remotive.io/api/remote-jobs

Found: one big table
Question came up: why pull this table apart to build it up later? --> benchmark idea. flat vs normalized. Storage, performance, complexity of querying

Insight: most companies don't put their data on a digestible API for open consumption. Often it's for integrating functinality, not for analytics workloads

Another reason for SCD (not sure if that could be through flat file still or needs normalization): the api only gives current job postings, no history. Often happens in the real world, if you want to do a historical analysis you will need the historical data. Most reliable is to keep track of that historical data yourself.

Start understanding the general outline of your dataset - postman / Python
Postman: - Total rows, rows per day, etc

Build a historical dataset - exploratory DA

	Future revision: load full dataset always
	Ingesting the full dataset daily is better
	üîÅ 1. Your DWH (with SCD2 logic) is designed to handle it
	üìà 2. Full ingest avoids edge cases
	‚ö°Ô∏è 3. Remotive isn‚Äôt that massive

		‚úÖ Best Practice (Recommended):

		Name the file/folder after the actual ingestion date (i.e. when you pulled the data), but keep the full dataset unchanged.

		Why?
			‚Ä¢	‚úîÔ∏è Truth in ingestion: You‚Äôre capturing the API as it is now, regardless of when the jobs were published.
			‚Ä¢	‚úîÔ∏è Simplicity: You don‚Äôt have to slice the dataset based on publication_date, which is fuzzy and inconsistent.
			‚Ä¢	‚úîÔ∏è DWH handles the dedup + changes anyway: Your DBT model (jobs_base) already tracks inserts, deletes, and changes with hashes & SCD2 logic.
			‚Ä¢	‚úîÔ∏è Future-proof: If you revisit this data in a month and the API format or backfill logic changed, you‚Äôll know exactly when you pulled what.

- Time considerations of the set
	- the api says it's 24 hours delayed
	- I'm in 3 april 2025 13:31 Paraguay time (utc-3) and the max timestamp of the dataset is 2025-04-02 08:51:41
	- this presumably shows that they don't upload full days by chunk, but some other system
	- to make this process clean, for now, let's take a daily dataset for full date of today - 2 days. So if we run today's DAG we'll get all entries for 2025-04-01. We'll be a bit delayed with data on our dashboard, but since timelyness is not very important here for real time decision making, that's a tradeoff I'll accept

	DAG: run at 6 utc -3 every morning. So that's 9 AM utc. I chose this time because my 13:31 Paraguay time (utc-3) - max timestamp of dataset 08:51:41 is about that time. This makes sure the dataset for full days remains intact and i won't cut off data. This is an assumption of course, and for a sensitive dataset this should be tested!

	! Document VERY well all things time related. I will name the folders after the contents of the data. So if i create a folder on 3 april, the folder name and contents will be that of 1 april. No april fools there :) Later in STA and in Airflow, it will become clear that the run of the script to ETL will actually be on 3 april

	‚Ä¢	‚úÖ Clean folder structure by data date
	‚Ä¢	‚úÖ Daily and historical splits handled
	‚Ä¢	‚úÖ CSV + JSON formats for flexibility
	‚Ä¢	‚úÖ Re-runnable scripts that overwrite intelligently
	‚Ä¢	‚úÖ Timezone-lag-aware logic built in (today - 2 / today - 3)

https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
Got docker running with Airflow, the DAG running the API script, writing the data both as json and csv to my files

Snowflake python connector

Bonus: How to Describe This in a Resume or Portfolio: ‚ÄúDesigned and implemented dimensional models in dbt with SCD Type 2 tracking for historical accuracy. Used star schemas to optimize Tableau dashboards over a Snowflake DWH.‚Äù That kind of phrasing hits DE, analytics, modeling, and BI in one go. üí•


DBT stuff

github search idea: path:dbt_project.yml schema:"

dbt run --profiles-dir ~/.dbt -m jobs_current












--------------
Benchmark ideas:

	‚Ä¢	Show performance differences in query structure
	‚Ä¢	Measure execution times between flat vs star
	‚Ä¢	Demonstrate storage impacts
	‚Ä¢	Compare ease of querying

üîπ Join Complexity Still Matters

You can benchmark:
	‚Ä¢	How many joins are needed
	‚Ä¢	Query readability and maintainability
	‚Ä¢	Performance of LIKE vs bridging tables for tags
	‚Ä¢	Whether GROUP BY performance changes on normalized vs flat models

-------------------

why hire data engineer?
- Set up pipelines for automatic data processing, to ingest data, combine different sources properly, increase data quality
- ETL?
- You want to increase your data process performance
- You want to decrease cost
- Historical data & changes are important to your business. And you want to make sure there's a robust system in place for that

---------------------

export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
nvm use default

export PATH="/Users/olivierdeswart/.nvm/versions/node/v20.17.0/bin:$PATH"

export COMPACT_HOME="/Users/olivierdeswart/my-binaries/compactc-macos"
export COMPACT_PATH="$COMPACT_HOME/lib/"
export ZKIR_PP="$COMPACT_HOME"

# pnpm
export PNPM_HOME="/Users/olivierdeswart/Library/pnpm"
case ":$PATH:" in
  *":$PNPM_HOME:"*) ;;
  *) export PATH="$PNPM_HOME:$PATH" ;;
esac
# pnpm end



[WARNING]: Console output during zsh initialization detected.

When using Powerlevel10k with instant prompt, console output during zsh
initialization may indicate issues.

You can:

  - Recommended: Change ~/.zshrc so that it does not perform console I/O
    after the instant prompt preamble. See the link below for details.

    * You will not see this error message again.
    * Zsh will start quickly and prompt will update smoothly.

  - Suppress this warning either by running p10k configure or by manually
    defining the following parameter:

      typeset -g POWERLEVEL9K_INSTANT_PROMPT=quiet

    * You will not see this error message again.
    * Zsh will start quickly but prompt will jump down after initialization.

  - Disable instant prompt either by running p10k configure or by manually
    defining the following parameter:

      typeset -g POWERLEVEL9K_INSTANT_PROMPT=off

    * You will not see this error message again.
    * Zsh will start slowly.

  - Do nothing.

    * You will see this error message every time you start zsh.
    * Zsh will start quickly but prompt will jump down after initialization.

For details, see:
https://github.com/romkatv/powerlevel10k#instant-prompt

-- console output produced during zsh initialization follows --

Now using node v20.17.0 (npm v10.8.2)


-------------
api research

https://github.com/public-apis/public-apis?tab=readme-ov-file

usa
data.gov
usaspending.gov

eu


tenders
https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/HORIZON-EIC-2025-PATHFINDERCHALLENGES-01-02?isExactMatch=true&status=31094501,31094502,31094503&order=DESC&pageNumber=1&pageSize=50&sortBy=startDate
