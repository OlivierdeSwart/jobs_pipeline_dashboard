20250403_notes

Blog concept: how to create a developer portfolio project

Think which skill(s) you want to showcase (DE)
Think of general idea (dashboard with job trends)
Think of tech stack, bonus points for review with AI (Airflow with Python, Snowflake DB, DBT, Tableau for dashboard)
	Also helpful to discuss repo/folder setup
Other things you want to showcase (star schema data modeling, STA/DWH/DM)
Take note of the questions you have along the way. Or you think other people might have
It's great if you can explain the "why" of what you're doing. (for me: performance and storage will be decreased. that's large amounts of money saved for data intensive companies. that's why data engineers have the right to exist in essence)

Start working. For me:
Find API with data: Remotive

https://remotive.io/api/remote-jobs

Found: one big table
Question came up: why pull this table apart to build it up later? --> benchmark idea. flat vs normalized. Storage, performance, complexity of querying

Insight: most companies don't put their data on a digestible API for open consumption. Often it's for integrating functinality, not for analytics workloads

Another reason for SCD (not sure if that could be through flat file still or needs normalization): the api only gives current job postings, no history. Often happens in the real world, if you want to do a historical analysis you will need the historical data. Most reliable is to keep track of that historical data yourself.

Start understanding the general outline of your dataset - postman / Python
Postman: - Total rows, rows per day, etc

Build a historical dataset - exploratory DA
- Time considerations of the set
	- the api says it's 24 hours delayed
	- I'm in 3 april 2025 13:31 Paraguay time (utc-3) and the max timestamp of the dataset is 2025-04-02 08:51:41
	- this presumably shows that they don't upload full days by chunk, but some other system
	- to make this process clean, for now, let's take a daily dataset for full date of today - 2 days. So if we run today's DAG we'll get all entries for 2025-04-01. We'll be a bit delayed with data on our dashboard, but since timelyness is not very important here for real time decision making, that's a tradeoff I'll accept

	DAG: run at 6 utc -3 every morning. So that's 9 AM utc. I chose this time because my 13:31 Paraguay time (utc-3) - max timestamp of dataset 08:51:41 is about that time. This makes sure the dataset for full days remains intact and i won't cut off data. This is an assumption of course, and for a sensitive dataset this should be tested!

	! Document VERY well all things time related. I will name the folders after the contents of the data. So if i create a folder on 3 april, the folder name and contents will be that of 1 april. No april fools there :) Later in STA and in Airflow, it will become clear that the run of the script to ETL will actually be on 3 april

	â€¢	âœ… Clean folder structure by data date
	â€¢	âœ… Daily and historical splits handled
	â€¢	âœ… CSV + JSON formats for flexibility
	â€¢	âœ… Re-runnable scripts that overwrite intelligently
	â€¢	âœ… Timezone-lag-aware logic built in (today - 2 / today - 3)

https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
Got docker running with Airflow, the DAG running the API script, writing the data both as json and csv to my files

Snowflake python connector

--------------
Benchmark ideas:

	â€¢	Show performance differences in query structure
	â€¢	Measure execution times between flat vs star
	â€¢	Demonstrate storage impacts
	â€¢	Compare ease of querying

ðŸ”¹ Join Complexity Still Matters

You can benchmark:
	â€¢	How many joins are needed
	â€¢	Query readability and maintainability
	â€¢	Performance of LIKE vs bridging tables for tags
	â€¢	Whether GROUP BY performance changes on normalized vs flat models

-------------------

why hire data engineer?
- Set up pipelines for automatic data processing, to ingest data, combine different sources properly, increase data quality
- ETL?
- You want to increase your data process performance
- You want to decrease cost
- Historical data & changes are important to your business. And you want to make sure there's a robust system in place for that

---------------------

export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
nvm use default

export PATH="/Users/olivierdeswart/.nvm/versions/node/v20.17.0/bin:$PATH"

export COMPACT_HOME="/Users/olivierdeswart/my-binaries/compactc-macos"
export COMPACT_PATH="$COMPACT_HOME/lib/"
export ZKIR_PP="$COMPACT_HOME"

# pnpm
export PNPM_HOME="/Users/olivierdeswart/Library/pnpm"
case ":$PATH:" in
  *":$PNPM_HOME:"*) ;;
  *) export PATH="$PNPM_HOME:$PATH" ;;
esac
# pnpm end
