20250403_notes

Fully orchestrated, cloud-deployed, auto-updating job intelligence platform with versioned data warehouse and public reporting layer

Blog concept: how to create a developer portfolio project

Think which skill(s) you want to showcase (DE)
Think of general idea (dashboard with job trends)
Think of tech stack, bonus points for review with AI (Airflow with Python, Snowflake DB, DBT, Tableau for dashboard)
	Also helpful to discuss repo/folder setup
Other things you want to showcase (star schema data modeling, STA/DWH/DM)
Take note of the questions you have along the way. Or you think other people might have
It's great if you can explain the "why" of what you're doing. (for me: performance and storage will be decreased. that's large amounts of money saved for data intensive companies. that's why data engineers have the right to exist in essence)

Start working. For me:
Find API with data: Remotive

https://remotive.io/api/remote-jobs

Found: one big table
Question came up: why pull this table apart to build it up later? --> benchmark idea. flat vs normalized. Storage, performance, complexity of querying

Insight: most companies don't put their data on a digestible API for open consumption. Often it's for integrating functinality, not for analytics workloads

Another reason for SCD (not sure if that could be through flat file still or needs normalization): the api only gives current job postings, no history. Often happens in the real world, if you want to do a historical analysis you will need the historical data. Most reliable is to keep track of that historical data yourself.

Start understanding the general outline of your dataset - postman / Python
Postman: - Total rows, rows per day, etc

Build a historical dataset - exploratory DA

	Future revision: load full dataset always
	Ingesting the full dataset daily is better
	üîÅ 1. Your DWH (with SCD2 logic) is designed to handle it
	üìà 2. Full ingest avoids edge cases
	‚ö°Ô∏è 3. Remotive isn‚Äôt that massive

		‚úÖ Best Practice (Recommended):

		Name the file/folder after the actual ingestion date (i.e. when you pulled the data), but keep the full dataset unchanged.

		Why?
			‚Ä¢	‚úîÔ∏è Truth in ingestion: You‚Äôre capturing the API as it is now, regardless of when the jobs were published.
			‚Ä¢	‚úîÔ∏è Simplicity: You don‚Äôt have to slice the dataset based on publication_date, which is fuzzy and inconsistent.
			‚Ä¢	‚úîÔ∏è DWH handles the dedup + changes anyway: Your DBT model (jobs_base) already tracks inserts, deletes, and changes with hashes & SCD2 logic.
			‚Ä¢	‚úîÔ∏è Future-proof: If you revisit this data in a month and the API format or backfill logic changed, you‚Äôll know exactly when you pulled what.

- Time considerations of the set
	- the api says it's 24 hours delayed
	- I'm in 3 april 2025 13:31 Paraguay time (utc-3) and the max timestamp of the dataset is 2025-04-02 08:51:41
	- this presumably shows that they don't upload full days by chunk, but some other system
	- to make this process clean, for now, let's take a daily dataset for full date of today - 2 days. So if we run today's DAG we'll get all entries for 2025-04-01. We'll be a bit delayed with data on our dashboard, but since timelyness is not very important here for real time decision making, that's a tradeoff I'll accept

	DAG: run at 6 utc -3 every morning. So that's 9 AM utc. I chose this time because my 13:31 Paraguay time (utc-3) - max timestamp of dataset 08:51:41 is about that time. This makes sure the dataset for full days remains intact and i won't cut off data. This is an assumption of course, and for a sensitive dataset this should be tested!

	! Document VERY well all things time related. I will name the folders after the contents of the data. So if i create a folder on 3 april, the folder name and contents will be that of 1 april. No april fools there :) Later in STA and in Airflow, it will become clear that the run of the script to ETL will actually be on 3 april

	‚Ä¢	‚úÖ Clean folder structure by data date
	‚Ä¢	‚úÖ Daily and historical splits handled
	‚Ä¢	‚úÖ CSV + JSON formats for flexibility
	‚Ä¢	‚úÖ Re-runnable scripts that overwrite intelligently
	‚Ä¢	‚úÖ Timezone-lag-aware logic built in (today - 2 / today - 3)

https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html
Got docker running with Airflow, the DAG running the API script, writing the data both as json and csv to my files

Snowflake python connector

üîÅ DB Ingest Pattern You Can Follow Universally
	1.	Research your DB engine
‚Üí ‚ÄúHow does [my database] recommend loading large volumes of data?‚Äù
	2.	Check connector capabilities
‚Üí For example, psycopg2 for Postgres has .copy_expert() and .execute_values().
	3.	Leverage optimized tools
‚Üí Often the vendor provides tools: bcp (SQL Server), mongoimport, gcloud bq load, etc.
	4.	Avoid row-by-row inserts
‚Üí Unless your dataset is extremely small or latency-critical (e.g., streaming 1 row/sec).
	5.	Use temp/intermediate storage if needed
‚Üí Like stages in Snowflake or staging tables in relational DBs.

If you‚Äôre ever switching to a new DB (e.g., ‚ÄúI want to move this pipeline to PostgreSQL‚Äù), just hit me with:
	‚ÄúHow do I optimize bulk inserts in [DB] from a local or API JSON source?‚Äù

Bonus: How to Describe This in a Resume or Portfolio: ‚ÄúDesigned and implemented dimensional models in dbt with SCD Type 2 tracking for historical accuracy. Used star schemas to optimize Tableau dashboards over a Snowflake DWH.‚Äù That kind of phrasing hits DE, analytics, modeling, and BI in one go. üí•


DBT stuff

github search idea: path:dbt_project.yml schema:"

dbt run --profiles-dir ~/.dbt -m jobs_current

dwh done

--==DM==--

	üí° Definition: Business Rule Logic

	A business rule defines or constrains a specific aspect of the business ‚Äî in data, it‚Äôs usually a transformation or filtering condition applied to reflect a real-world rule.

	In your case:

	‚ÄúExtract salary min, max, average, and bonus presence from a messy string.‚Äù
	That‚Äôs business rule logic in action: converting human-language rules into reproducible code.

	‚∏ª

	üß† What we want now

	From a string like:

	"$95,000 - $110,000 + $5,000 performance-based bonus"

	We want to extract:

	Column Name
	Value (example)
	salary_min_usd
	95000
	salary_max_usd
	110000
	salary_avg_usd
	102500
	has_bonus_component
	TRUE

	üß™ Let‚Äôs break down the logic:
		1.	Find numbers: Use regex to extract all numeric values
		2.	Take first two numbers: Treat as min and max
		3.	Average: (min + max) / 2
		4.	Detect bonus: Look for words like bonus, performance, commission, etc.






AWS integration

policy name: snowflake_integration_policy

aws s3 sync airflow/mwaa-deploy s3://olivier-mwaa/dags --delete

https://docs.aws.amazon.com/mwaa/latest/userguide/samples-dbt.html


































--------------
Benchmark ideas:

	‚Ä¢	Show performance differences in query structure
	‚Ä¢	Measure execution times between flat vs star
	‚Ä¢	Demonstrate storage impacts
	‚Ä¢	Compare ease of querying

üîπ Join Complexity Still Matters

You can benchmark:
	‚Ä¢	How many joins are needed
	‚Ä¢	Query readability and maintainability
	‚Ä¢	Performance of LIKE vs bridging tables for tags
	‚Ä¢	Whether GROUP BY performance changes on normalized vs flat models

-------------------

why hire data engineer?
- Set up pipelines for automatic data processing, to ingest data, combine different sources properly, increase data quality
- ETL?
- You want to increase your data process performance
- You want to decrease cost
- Historical data & changes are important to your business. And you want to make sure there's a robust system in place for that

---------------------

export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion
nvm use default

export PATH="/Users/olivierdeswart/.nvm/versions/node/v20.17.0/bin:$PATH"

export COMPACT_HOME="/Users/olivierdeswart/my-binaries/compactc-macos"
export COMPACT_PATH="$COMPACT_HOME/lib/"
export ZKIR_PP="$COMPACT_HOME"

# pnpm
export PNPM_HOME="/Users/olivierdeswart/Library/pnpm"
case ":$PATH:" in
  *":$PNPM_HOME:"*) ;;
  *) export PATH="$PNPM_HOME:$PATH" ;;
esac
# pnpm end



[WARNING]: Console output during zsh initialization detected.

When using Powerlevel10k with instant prompt, console output during zsh
initialization may indicate issues.

You can:

  - Recommended: Change ~/.zshrc so that it does not perform console I/O
    after the instant prompt preamble. See the link below for details.

    * You will not see this error message again.
    * Zsh will start quickly and prompt will update smoothly.

  - Suppress this warning either by running p10k configure or by manually
    defining the following parameter:

      typeset -g POWERLEVEL9K_INSTANT_PROMPT=quiet

    * You will not see this error message again.
    * Zsh will start quickly but prompt will jump down after initialization.

  - Disable instant prompt either by running p10k configure or by manually
    defining the following parameter:

      typeset -g POWERLEVEL9K_INSTANT_PROMPT=off

    * You will not see this error message again.
    * Zsh will start slowly.

  - Do nothing.

    * You will see this error message every time you start zsh.
    * Zsh will start quickly but prompt will jump down after initialization.

For details, see:
https://github.com/romkatv/powerlevel10k#instant-prompt

-- console output produced during zsh initialization follows --

Now using node v20.17.0 (npm v10.8.2)


-------------
api research

https://github.com/public-apis/public-apis?tab=readme-ov-file

usa
data.gov
usaspending.gov

eu


tenders
https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/HORIZON-EIC-2025-PATHFINDERCHALLENGES-01-02?isExactMatch=true&status=31094501,31094502,31094503&order=DESC&pageNumber=1&pageSize=50&sortBy=startDate

------------

stack actuality:

Great question ‚Äî let‚Äôs break down the adoption and popularity of the three pillars in your stack (Snowflake, Airflow, Tableau) compared to alternatives over the past 10 years, based on industry trends, surveys, usage reports, job postings, and market share insights.

‚∏ª

üßä 1. Snowflake vs Other Databases

Snowflake Popularity & Growth
	‚Ä¢	Launch Year: 2014, became widely adopted around 2018+.
	‚Ä¢	Market Position: One of the fastest-growing cloud data platforms ever.
	‚Ä¢	Gartner Magic Quadrant (2020‚Äì2024): Leader in cloud data warehousing.
	‚Ä¢	Key Advantages: Separation of storage/compute, performance scaling, SQL-native, built-in security, SaaS model.

Market Share Estimate (as of 2024):

Database	Approx. Usage Share in DE Projects
Snowflake	~20‚Äì30% and rapidly growing
BigQuery (GCP)	~15%
Redshift (AWS)	~10%
Azure Synapse	~10%
PostgreSQL / MySQL	~20‚Äì25% (mostly for app + DE blend)
Databricks (Delta)	~10% (esp. ML/streaming heavy)

‚úÖ Conclusion: Snowflake is top-tier, especially in modern, cloud-native stacks. Its market share among new data projects is strong and growing.

‚∏ª

‚öôÔ∏è 2. Airflow vs Other Orchestration Tools

Airflow
	‚Ä¢	Initial Release: 2015 by Airbnb, now part of the Apache Foundation.
	‚Ä¢	Strengths: Python-native, highly customizable, strong open-source community.
	‚Ä¢	Used by: Airbnb, Stripe, Lyft, Slack, many modern data teams.

Alternatives & Niche Tools:

Tool	Popularity Notes
Airflow	~60% usage share; industry standard.
Luigi	Pre-Airflow, now outdated.
Prefect	Modern, better UX, catching on (~10‚Äì15%)
Dagster	Declarative + type-safe, also rising (~5‚Äì10%)
dbt Cloud jobs	Simpler scheduling use case only
Managed Airflow	On GCP, AWS MWAA, Astronomer Cloud

‚úÖ Conclusion: Airflow dominates orchestration for serious DE projects. Some orgs use managed Airflow or switch to Prefect/Dagster for friendlier DX, but Airflow remains the gold standard.

‚∏ª

üìä 3. Tableau vs Other BI Tools

Tableau
	‚Ä¢	Founded: 2003, acquired by Salesforce in 2019.
	‚Ä¢	Pros: Intuitive interface, great for dashboards, strong community.
	‚Ä¢	Cons: Desktop-heavy, weaker on semantic layers vs competitors.

Market Share Comparison (2023‚Äì2024)

Tool	Approx. Market Share
Power BI	~35%
Tableau	~25%
Looker	~10‚Äì15%
Metabase	~5% (open source)
Mode	~5%
Superset	~3‚Äì5%

‚úÖ Conclusion: Tableau is still a top 2 BI tool, especially in enterprises and data teams with analysts. It remains one of the most commonly listed tools in job postings.

‚∏ª

üíº Summary: Your Stack

Component	Tool	Status
DB/warehouse	Snowflake	üöÄ Industry-leading cloud DWH
Orchestration	Airflow	üèÜ Standard for pipelines
BI/Viz	Tableau	ü•à Top 2 enterprise BI tool

‚úÖ Your stack is mainstream, widely adopted, and highly employable. It represents a modern cloud-native data platform that many companies are moving toward ‚Äî especially in mid-size to enterprise settings.

Want me to help you visualize this comparison or turn it into a blog post or slide?

---------
